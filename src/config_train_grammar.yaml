# Grammar-based tokenization training config
# Uses production rules (55 vocab), max_length=120

data:
  prod_path: 'examples_out/prod_48444_ids_int16.npy'
  masks_path: 'examples_out/prod_48444_masks_packed.npy'
  validation_split: 0.1

training:
  batch_size: 256
  epochs: 1000
  learning_rate: 0.0001
  clip: 5.0
  early_stopping_patience: 50  # Increased to catch best before collapse
  beta: 0.0001  # 1e-4 KL weight
  monitor_metric: "val/seq_acc"  # Focus on reconstruction
  mode: "max"  # Maximize seq_acc
  device: "gpu"
  accelerator: "gpu"
  num_workers: 4

model:
  encoder:
    hidden_size: 256
    conv_sizes: [64, 128, 256]
    kernel_sizes: [2, 3, 4]
    use_batch_norm: true
    dropout: 0.1
  
  decoder:
    mode: "positional"
    hidden_size: 501
    num_layers: 2
    use_layer_norm: false
    dropout: 0.1
    rnn_type: 'gru'
  
  shared:
    z_dim: 74
    max_length: 114  # Updated: max sequence length in new dataset
    vocab_size: 53   # Updated: production rules (removed SIGN rules)
    pad_idx: -1

saving:
  base_dir: "checkpoints/grammar_based"
  checkpoint_pattern: "grammar-beta1e4-{epoch:03d}-seqacc={val/seq_acc:.4f}"
  save_top_k: 5  # Keep top 5 checkpoints
  logs_dir: "logs"
  run_name: "grammar_beta1e4_reconstruction"
