# VAE training config for the operator-only 48000 dataset (16 families x 3000).

model:
  z_dim: 26
  encoder:
    hidden_size: 128
    conv_sizes: [64, 128, 256]
    kernel_sizes: [7, 7, 7]
    dropout: 0.1
  decoder:
    hidden_size: 80
    num_layers: 3
    dropout: 0.1
    rnn_type: 'gru'
    bidirectional: false

data:
  # Produced by:
  #   python3 dataset_creation/create_tokenized_data.py \
  #     --dataset data/raw/pde_dataset_48000_fixed.csv \
  #     --splits data/splits_48000_fixed \
  #     --output data/tokenized_48000_fixed
  grammar_ids_path: 'data/tokenized_48000_fixed/grammar_full.npy'
  grammar_masks_path: 'data/tokenized_48000_fixed/grammar_full_masks.npy'
  token_ids_path: 'data/tokenized_48000_fixed/token_full.npy'
  token_masks_path: 'data/tokenized_48000_fixed/token_full_masks.npy'
  split_dir: 'data/splits_48000_fixed'

  grammar_max_length: 114
  grammar_vocab_size: 56
  token_max_length: 62
  token_vocab_size: 82

training:
  batch_size: 256
  epochs: 1000
  learning_rate: 0.001
  clip: 1.0
  early_stopping_patience: 100
  beta: 0.001
  device: "gpu"
  accelerator: "gpu"
  num_workers: 4
  seed: 42
  lr_scheduler:
    type: "plateau"
    patience: 10
    factor: 0.5
    min_lr: 1e-6
    T_max: null
  kl_annealing:
    beta: 0.001
    free_bits: 0.5
    anneal_epochs: 0
    cyclical: false
    cycle_epochs: 10

grammar:
  data:
    prod_path: 'data/tokenized_48000_fixed/grammar_full.npy'
    masks_path: 'data/tokenized_48000_fixed/grammar_full_masks.npy'
    split_dir: 'data/splits_48000_fixed'
  model:
    max_length: 114
    vocab_size: 56
    pad_idx: -1
  saving:
    checkpoint_dir: "checkpoints_48000_fixed/grammar_vae"
    logs_dir: "logs"
    save_top_k: 1

token:
  data:
    token_path: 'data/tokenized_48000_fixed/token_full.npy'
    masks_path: 'data/tokenized_48000_fixed/token_full_masks.npy'
    split_dir: 'data/splits_48000_fixed'
  model:
    max_length: 62
    vocab_size: 82
    pad_idx: -1
  saving:
    checkpoint_dir: "checkpoints_48000_fixed/token_vae"
    logs_dir: "logs"
    save_top_k: 1

# Used by the actual training entrypoint (`vae/train/train.py`) for checkpoint root.
logging:
  checkpoint_dir: "checkpoints_48000_fixed"
  log_dir: "logs_48000_fixed"
  save_top_k: 1
  log_every_n_steps: 50

