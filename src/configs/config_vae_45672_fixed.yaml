# VAE training config for the regenerated, physics-consistent 45672 dataset.
#
# This keeps the original pipeline intact but writes to *separate* data/checkpoint dirs
# so you don't overwrite old runs.

model:
  z_dim: 26
  encoder:
    hidden_size: 128
    conv_sizes: [64, 128, 256]
    kernel_sizes: [7, 7, 7]
    dropout: 0.1
  decoder:
    hidden_size: 80
    num_layers: 3
    dropout: 0.1
    rnn_type: 'gru'
    bidirectional: false

data:
  # Tokenized arrays produced by:
  #   python3 dataset_creation/create_tokenized_data.py --dataset data/raw/pde_dataset_45672_fixed.csv --splits data/splits_45672_fixed --output data/tokenized_45672_fixed
  grammar_ids_path: 'data/tokenized_45672_fixed/grammar_full.npy'
  grammar_masks_path: 'data/tokenized_45672_fixed/grammar_full_masks_packed.npy'
  token_ids_path: 'data/tokenized_45672_fixed/token_full.npy'
  token_masks_path: 'data/tokenized_45672_fixed/token_full_masks.npy'
  split_dir: 'data/splits_45672_fixed'

  grammar_max_length: 114
  grammar_vocab_size: 55
  token_max_length: 62
  token_vocab_size: 82

training:
  batch_size: 256
  epochs: 1000
  learning_rate: 0.001
  clip: 1.0
  early_stopping_patience: 100
  beta: 0.001
  device: "gpu"
  accelerator: "gpu"
  num_workers: 4
  seed: 42
  lr_scheduler:
    type: "plateau"
    patience: 10
    factor: 0.5
    min_lr: 1e-6
    T_max: null
  kl_annealing:
    beta: 0.001
    free_bits: 0.5
    anneal_epochs: 0
    cyclical: false
    cycle_epochs: 10

grammar:
  data:
    prod_path: 'data/tokenized_45672_fixed/grammar_full.npy'
    masks_path: 'data/tokenized_45672_fixed/grammar_full_masks_packed.npy'
    split_dir: 'data/splits_45672_fixed'
  model:
    max_length: 114
    vocab_size: 55
    pad_idx: -1
  saving:
    checkpoint_dir: "checkpoints_45672_fixed/grammar_vae"
    logs_dir: "logs"
    save_top_k: 1

token:
  data:
    token_path: 'data/tokenized_45672_fixed/token_full.npy'
    masks_path: 'data/tokenized_45672_fixed/token_full_masks.npy'
    split_dir: 'data/splits_45672_fixed'
  model:
    max_length: 62
    vocab_size: 82
    pad_idx: -1
  saving:
    checkpoint_dir: "checkpoints_45672_fixed/token_vae"
    logs_dir: "logs"
    save_top_k: 1

