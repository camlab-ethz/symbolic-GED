# =============================================================================
# VAE Configuration
# =============================================================================
# This is the main configuration file for VAE training.
# All settings are organized into logical sections.
#
# Usage:
#   python -m vae.train --config config.yaml --tokenization grammar
#
# CLI overrides take precedence over this file.
# =============================================================================

# Tokenization method: 'grammar' or 'token'
tokenization: grammar

# Random seed for reproducibility (null for no seeding)
seed: 42

# =============================================================================
# Model Architecture
# =============================================================================
model:
  # Latent space dimension
  z_dim: 26
  
  # Encoder (Convolutional)
  encoder:
    hidden_size: 128
    conv_sizes: [64, 128, 256]
    kernel_sizes: [7, 7, 7]
    dropout: 0.1
  
  # Decoder (GRU-based)
  decoder:
    hidden_size: 80
    num_layers: 3
    dropout: 0.1
    bidirectional: false
    rnn_type: gru

# =============================================================================
# Training Settings
# =============================================================================
training:
  batch_size: 256
  max_epochs: 1000
  lr: 0.001
  gradient_clip: 1.0
  early_stopping_patience: 20
  num_workers: 4
  
  # KL Divergence Annealing
  kl_annealing:
    beta: 0.0001          # Final KL weight
    free_bits: 0.5        # Minimum KL per dimension
    anneal_epochs: 0      # Linear annealing epochs (0 = no annealing)
    cyclical: false       # Use cyclical annealing
    cycle_epochs: 10      # Epochs per cycle
  
  # Learning Rate Scheduler
  lr_scheduler:
    type: plateau         # Options: null, plateau, cosine, step, exponential
    patience: 10          # Epochs to wait before reducing LR
    factor: 0.5           # LR reduction factor
    min_lr: 0.000001      # Minimum learning rate
    T_max: null           # For cosine scheduler (null = use max_epochs)

# =============================================================================
# Data Paths
# =============================================================================
data:
  # Grammar tokenization
  grammar_ids_path: examples_out/prod_48444_ids_int16_clean.npy
  grammar_masks_path: examples_out/prod_48444_masks_clean.npy
  grammar_max_length: 114
  grammar_vocab_size: 53
  
  # Token tokenization
  token_ids_path: examples_out/token_48444_ids_int16_clean.npy
  token_masks_path: examples_out/token_48444_masks_clean.npy
  token_max_length: 62
  token_vocab_size: 82
  
  # Train/val/test splits
  split_dir: data_splits

# =============================================================================
# Logging & Checkpoints
# =============================================================================
logging:
  checkpoint_dir: checkpoints
  log_dir: logs
  save_top_k: 1
  log_every_n_steps: 50
  print_every: 0          # Detailed prints every N steps (0 = disabled)
