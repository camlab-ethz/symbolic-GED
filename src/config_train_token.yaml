# Token-based (Lample & Charton) tokenization training config
# Uses character tokens (82 vocab), max_length=62

data:
  prod_path: 'examples_out/token_48444_ids_int16.npy'
  masks_path: 'examples_out/token_48444_masks_bool.npy'
  validation_split: 0.1

training:
  batch_size: 256
  epochs: 1000
  learning_rate: 0.0001
  clip: 5.0
  early_stopping_patience: 50  # Increased to catch best performance
  beta: 0.0001  # 1e-4 KL weight (matching grammar)
  monitor_metric: "val/seq_acc"  # Focus on reconstruction
  mode: "max"  # Maximize seq_acc
  device: "gpu"
  accelerator: "gpu"
  num_workers: 4

model:
  encoder:
    hidden_size: 256
    conv_sizes: [64, 128, 256]
    kernel_sizes: [2, 3, 4]
    use_batch_norm: true
    dropout: 0.1
  
  decoder:
    mode: "positional"
    hidden_size: 501
    num_layers: 2
    use_layer_norm: false
    dropout: 0.1
    rnn_type: 'gru'
  
  shared:
    z_dim: 74
    max_length: 62   # Token sequences are shorter
    vocab_size: 82   # Character tokens
    pad_idx: -1

saving:
  base_dir: "checkpoints/token_based"
  checkpoint_pattern: "token-beta1e4-{epoch:03d}-seqacc={val/seq_acc:.4f}"
  save_top_k: 5  # Keep top 5 checkpoints
  logs_dir: "logs"
  run_name: "token_beta1e4_reconstruction"
