# Unified VAE Configuration
# Same architecture for both grammar and token tokenization methods
# Only representation-specific settings differ (max_length, vocab_size, data paths)

# ==================================================================
# SHARED VAE ARCHITECTURE (Same for both tokenization methods)
# ==================================================================
model:
  z_dim: 26  # Latent dimension
  encoder:
    hidden_size: 128
    conv_sizes: [64, 128, 256]
    kernel_sizes: [7, 7, 7]
    dropout: 0.1
  decoder:
    hidden_size: 80
    num_layers: 3
    dropout: 0.1
    rnn_type: 'gru'
    bidirectional: false

# ==================================================================
# SHARED DATA SETTINGS
# ==================================================================
data:
  grammar_masks_path: 'data/tokenized/grammar_full_masks_packed.npy'
  token_masks_path: 'data/tokenized/token_full_masks.npy'

# ==================================================================
# SHARED TRAINING SETTINGS
# ==================================================================
training:
  batch_size: 256
  epochs: 1000
  learning_rate: 0.001
  clip: 1.0
  early_stopping_patience: 100  # Increased from 20 to allow seq_acc to improve (starts at 0%)
  beta: 0.001  # 1e-3 KL weight (10x higher for better latent space structure)
  device: "gpu"
  accelerator: "gpu"
  num_workers: 4
  seed: 42  # For reproducibility
  # Learning Rate Scheduling
  lr_scheduler:
    type: "plateau"  # Options: null, "plateau", "cosine", "step", "exponential"
    patience: 10  # Epochs to wait before reducing LR (for plateau/step)
    factor: 0.5  # Factor to reduce LR by
    min_lr: 1e-6  # Minimum learning rate
    T_max: null  # Max epochs for cosine (null = use training epochs)
  # KL Annealing
  kl_annealing:
    beta: 0.001  # Final/max KL weight (matches training.beta)
    free_bits: 0.5  # Minimum KL per dimension
    anneal_epochs: 0  # Linear annealing epochs (0 = no annealing)
    cyclical: false  # Use cyclical annealing
    cycle_epochs: 10  # Epochs per cycle
# ==================================================================
# GRAMMAR-SPECIFIC SETTINGS
# ==================================================================
grammar:
  data:
    prod_path: 'data/tokenized/grammar_full.npy'
    masks_path: 'data/tokenized/grammar_full_masks_packed.npy'
    split_dir: 'data/splits'
  
  model:
    max_length: 114  # Grammar production sequences
    vocab_size: 55   # Number of production rules (P) - updated for Cahn-Hilliard support
    pad_idx: -1
  
  saving:
    checkpoint_dir: "checkpoints/grammar_vae"
    logs_dir: "logs"
    save_top_k: 1  # Keep only best model

# ==================================================================
# TOKEN-SPECIFIC SETTINGS (Lample & Charton)
# ==================================================================
token:
  data:
    token_path: 'data/tokenized/token_full.npy'
    masks_path: 'data/tokenized/token_full_masks.npy'
    split_dir: 'data/splits'
  
  model:
    max_length: 62   # Token sequences (shorter than grammar)
    vocab_size: 82   # Character tokens
    pad_idx: -1
  
  saving:
    checkpoint_dir: "checkpoints/token_vae"
    logs_dir: "logs"
    save_top_k: 1  # Keep only best model
